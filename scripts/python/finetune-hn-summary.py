#!/usr/bin/env python3
"""
Fine-tune AI models for summarizing HN comments using Unsloth

This script contains the code to fine-tune large language models for the task of
summarizing Hacker News comments. The script:
1. Sets up environment and dependencies 
2. Loads and prepares training data from HuggingFace
3. Configures and initializes the base model
4. Fine-tunes using LoRA adapters
5. Exports the model to Ollama format for inference

The original training data comes from summaries generated by top models like GPT-4,
Claude and similar high-end models. We fine-tune smaller models on this data to get
improved performance while reducing costs.
"""

import os
import json
import subprocess
import time
from dotenv import load_dotenv
from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments, TextStreamer

# Load environment variables
load_dotenv()

def initialize_model(base_model_name="unsloth/llama-3-8b-bnb-4bit", max_seq_length=4096):
    """Initialize the base model and tokenizer"""
    print(f"\nInitializing base model: {base_model_name}")

    fourbit_models = [
        "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
        "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
        "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
        "unsloth/llama-3-8b-Instruct-bnb-4bit",
        "unsloth/llama-3-70b-bnb-4bit",
        "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
        "unsloth/Phi-3-medium-4k-instruct",
        "unsloth/mistral-7b-bnb-4bit",
        "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
        "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"
    ] # More models at https://huggingface.co/unsloth
    
    # Model initialization parameters
    dtype = None  # None for auto detection
    load_in_4bit = True  # Use 4bit quantization to reduce memory usage
    
    # Initialize base model and tokenizer
    base_model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=base_model_name,
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit
    )
    
    return base_model, tokenizer

def setup_lora_adapter(base_model):
    """Set up LoRA adapters for efficient fine-tuning"""
    lora_model = FastLanguageModel.get_peft_model(
        base_model,
        r=16,  # Rank of the fine-tuning process
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj",
        ],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
        use_rslora=False,
        loftq_config=None,
    )
    return lora_model

def load_training_data(dataset_id="annjose/hn-comments-small", hf_token=None):
    """Load the training dataset from HuggingFace"""
    # Load dataset splits
    train_dataset = load_dataset(dataset_id, split="train", token=hf_token)
    val_dataset = load_dataset(dataset_id, split="validation", token=hf_token) 
    test_dataset = load_dataset(dataset_id, split="test", token=hf_token)
    
    print(f"Dataset sizes:")
    print(f"Train: {len(train_dataset)}")
    print(f"Validation: {len(val_dataset)}")
    print(f"Test: {len(test_dataset)}")
    
    return train_dataset, val_dataset, test_dataset

def format_training_data(dataset, tokenizer):
    """Format the dataset according to model's template"""
    # Define prompts and templates
    system_prompt = """You are an AI assistant specialized in analyzing and summarizing Hacker News discussions.
A discussion consists of threaded comments where each comment can have child comments (replies) nested underneath it,
forming interconnected conversation branches. Your task is to provide concise, meaningful summaries that capture the
essence of the discussion while prioritizing engaging and high quality content."""

    user_prompt_prefix = "This is your input:\n The title of the post and comments are separated by dashed lines."
    
    llama_template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>

{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{OUTPUT}<|eot_id|>"""

    def format_prompts_func(examples):
        try:
            post_ids = examples["post_id"]
            input_comments = examples["input_comment"]
            summaries = examples["output_summary"]

            formatted_texts = []
            for post_id, comment, summary in zip(post_ids, input_comments, summaries):
                if not all([comment, summary]):
                    continue

                user_prompt = f"{user_prompt_prefix}\n{comment}"
                formatted_text = llama_template.format(
                    SYSTEM=system_prompt,
                    INPUT=user_prompt,
                    OUTPUT=summary
                ) + tokenizer.eos_token

                formatted_texts.append(formatted_text)

            return {"text": formatted_texts}
        except Exception as e:
            print(f"Error formatting prompts: {e}")
            raise

    # Format the dataset
    formatted_dataset = dataset.map(format_prompts_func, batched=True)
    return formatted_dataset

def setup_trainer(lora_model, tokenizer, dataset, max_seq_length=4096):
    """Set up the SFT trainer with training arguments"""
    current_run_name = "llama_4096_6posts_below_4k_tokensize"
    
    trainer = SFTTrainer(
        model=lora_model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        dataset_num_proc=2,
        packing=False,
        args=TrainingArguments(
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=5,
            max_steps=60,
            learning_rate=2e-4,
            fp16=not FastLanguageModel.is_bfloat16_supported(),
            bf16=FastLanguageModel.is_bfloat16_supported(),
            logging_steps=1,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs",
            report_to="wandb",
            run_name=current_run_name,
        ),
    )
    return trainer

def export_model(model, tokenizer, output_dir="model"):
    """Export the model to GGUF format"""
    print("Exporting model to GGUF format...")
    model.save_pretrained_gguf(output_dir, tokenizer)

def setup_ollama():
    """Initialize Ollama server"""
    # Start Ollama server
    subprocess.Popen(["ollama", "serve"])
    time.sleep(3)  # Wait for server to start
    
    # Create Ollama model
    subprocess.run(["ollama", "create", "unsloth_model", "-f", "./model/Modelfile"])

def main():
    # Get HuggingFace token
    hf_token = os.environ.get('HF_TOKEN')
    if not hf_token:
        raise ValueError("HF_TOKEN environment variable not set")
    
    # Initialize model
    base_model, tokenizer = initialize_model()
    
    # Setup LoRA adapter
    lora_model = setup_lora_adapter(base_model)
    
    # Load and format training data
    train_dataset, val_dataset, test_dataset = load_training_data(hf_token=hf_token)
    formatted_dataset = format_training_data(train_dataset, tokenizer)
    
    # Setup trainer
    trainer = setup_trainer(lora_model, tokenizer, formatted_dataset)
    
    # Train model
    print("Starting training...")
    trainer_stats = trainer.train()
    print(f"Training completed in {trainer_stats.metrics['train_runtime']} seconds")
    
    # Export model
    export_model(lora_model, tokenizer)
    
    # Setup Ollama integration
    setup_ollama()
    
    print("Model training and export completed successfully!")

if __name__ == "__main__":
    main()